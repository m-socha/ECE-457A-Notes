\documentclass[12pt,titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}

\begin{document}
  \begin{titlepage}
    \vspace*{\fill}
    \centering

    \textbf{\Huge ECE 457A Course Notes} \\ [0.4em]
    \textbf{\Large Cooperative and Adaptive Algorithms} \\ [1em]
    \textbf{\Large Michael Socha} \\ [1em]
    \textbf{\large 4A Software Engineering} \\
    \textbf{\large University of Waterloo} \\
    \textbf{\large Spring 2018} \\
    \vspace*{\fill}
  \end{titlepage}

  \newpage 

  \tableofcontents

  \newpage

  \section{Course Overview}
    \subsection{Logistics}
      \begin{itemize}
        \item \textbf{Professor:} Allaa Hilal
        \item \textbf{Email:} ahilal@uwaterloo.ca
        \item \textbf{Office:} EIT-3135
      \end{itemize}

    \subsection{Overview of Topics}
      This is a course that approaches artificial intelligence by examining various algorithms that adapt to their
      environment and cooperate with one another. Topics include:
      \begin{itemize}
        \item Algorithms that exhibit intelligent behavior
        \item Concepts of adaptation and cooperation between algorithms
        \item Meta-heuristics, evolutionary computing methods, swarm intelligence, ant-colony algorithms, and particle
        swarm methods
        \item Apply the above ideas to solve various (often ill-structured) continuous and discrete problems
      \end{itemize}

  \section{Introduction}

    \subsection{What is Artificial Intelligence (AI)?}
      Intelligence is the ability to acquire and apply knowledge and skills. AI is the science of creating intelligent machines,
      including intelligent computer programs. Sample applications include visual perception, speech recognition and various
      forms of decision-making

      \subsubsection{AI vs Machine Learning vs Deep Learning vs Data Science}
        The above terms are often thrown around almost interchangealbly, but they refer to different things. Deep learning is
        a type of machine learning, which is a type of AI. Data science is a separate field that has some overlap with these
        three other concepts.

      \subsubsection{Inspiration from Nature}
        Some of the inspirations for intelligent systems come from the natural world. Sample inspirations include ant path-finding,
        bird flocking, and fish schooling.

    \subsection{Thinking Rationally vs Behaving Rationally}
      In rational thinking, logical systems are used to achieve goals via inferencing. It can be hard to represent informal knowledge,
      and not all problems can be solved through such methods (e.g. problems with uncertainty).

      In rational behavior, a perceives its environment and acts to achieve goals according to some set of beliefs. This is a more
      general approach than inferencing, and actions taken to achieve a goal may not necessarily be optimal or ``correct'', but
      may be an acceptable solution anyways.
      
    \subsection{Swarm Intelligence}
      Swarm intelligence is an AI technique that builds systems based on the collective behavior of decentralized, self-organized units.
      There are no centralized control structures, with agents only interacting with each other and the environment.

    \subsection{Intelligent Agents}
      An agent is something that senses its environment and acts on the collected information. A rational agent acts in a way that is
      expected to maximize performance on the basis of perceptual history and built-in knowledge. Types of agents include:
      \begin{itemize}
        \item \textbf{Simple Reflex Agents:} Follow a lookup-table approach; needs fully observable environment
        \item \textbf{Model-Based Reflex Agents:} Add state information to handle partially observable environments
        \item \textbf{Goal-Based Agents:} Add concept of goals to help choose actions
        \item \textbf{Utility-Based Agents:} Add utility to decide ``good'' or ``'bad'' when faced with conflicting goals
        \item \textbf{Learning Agents:} Add ability to learn from experience to improve performance
      \end{itemize}

    \subsection{Environments}
      The environments in which agents operate influence the design of the agents. Types of environments include:
      \begin{itemize}
        \item \textbf{Fully vs Partially Observable:} In fully observable environments, sensors can detect all aspects
        relevant to choice of action. This is not the case in partially observable environments, which may exist because
        of missing information or inaccurate sensors.
        \item \textbf{Deterministic vs Stochastic:} Deterministic environments are only influenced by their current
        state and the next action executed by the agent; otherwise, the environment is stochastic.
        \item \textbf{Episodic vs Sequential:} In episodic environments, the choice of action in each episode does not
        depend on previous episodes, unlike in sequential environments, where an agent needs to ``think ahead''.
        \item \textbf{Static vs Dynamic:} Environments where the state may change while the agent is deliberating are
        considered dynamic; other environments are considered static.
        \item \textbf{Discrete vs Continuous:} A discrete/continuous distinction can be applied to various aspects of
        an environment, including the way time flow is handled and to the actions of the agent.
        \item \textbf{Single vs Multi Agent}
      \end{itemize}

    \subsection{Adaptive and Cooperative Algorithms}
      Adaptive algorithms are able to change their behavior as they are run. Cooperative algorithms algorithms work
      together to solve a joint problem, communicating either directly or indirectly with one another.

  \section{Search Problem Formulation}
    The first step in solving a search problem lies in formulation. Sources of potential complexity in formalizing and
    solving real-world search problems include multimodality, lack of a mathematical model, non-differentiability, and
    a combinatoric or distributed nature.

    \subsection{Well-Structured vs Ill-Structured Problems}
      In well-structured problems, the existing and end state are well-defined, and so are the actions that can be taken
      to change state. In ill-structured problems, the existing state is well-defined, but the end state and actions to
      change state are typically undefined.

    \subsection{Optimization Problems}
      Optimization problems concern finding the ``best'' solution from a set of solutions subject to a set of constraints.
      Optimization techniques are search methods where the goal is to find a solution to an optimization problem. Optimization
      techniques can be divided into exact algorithms, where an optimal solution is found but often at a very high computational
      cost, and approximate solutions, where a near-optimal solution is found at a far lower computational cost.

      \subsubsection{Approximation Methods}
        Due to their relatively low computational costs, approximation methods are often preferred for solving problems of a
        combinatorial nature where heuristics can be applied to find near-optimal solutions. Approaches to approximation can
        be divided into constructive methods, where a solution is built up one component at a time, and local search methods,
        where actions are iteratively applied to some initial solution to attempt to improve it.

    \subsection{Goal and Problem Formulation}
      In goal formulation, decisions are made on which aspects of a problem are important and which can be ignored. Problem
      formulation concerns determining how to manipulate these important aspects. Goal formulation followed by problem
      formulation yields the following information:
      \begin{itemize}
        \item \textbf{State space:} Complete or partial configuration of a problem
        \item \textbf{Initial state:} State in which search begins
        \item \textbf{Goal state:} State in which search ends
        \item \textbf{Action set:} Set of possible transitions between states
        \item \textbf{Cost:} Can be used to compare various solutions
      \end{itemize}

      Generally speaking, a closed-world assumption can be followed, meaning that each state is a complete description of the world.
      Environments in closed-world problems are typically fully observable, deterministic, sequential, static, and discrete.

    \subsection{Properties of Search Algorithms}
      The key properties of search algorithms are:
      \begin{itemize}
        \item \textbf{Completeness:} Whether algorithm is guaranteed to find goal node, provided one exists
        \item \textbf{Optimality:} Whether algorithm is guaranteed to find the best goal node (i.e. one with cheapest cost)
        \item \textbf{Time complexity:} How many nodes generated
        \item \textbf{Space complexity:} Maximum number of nodes stored in memory
      \end{itemize}

  \section{Graph Search Algorithms}

    \subsection{Search Tree Terminology}
      \begin{itemize}
        \item \textbf{Node:} Represents a state in a search problem
        \item \textbf{Edge:} Represents a transition between states
        \item \textbf{Branching factor (b):} Maximum number of child nodes extending from a parent node
        \item \textbf{Maximum depth (m):} Number edges in the shortest path from the root node to the furthest node.
        Variable $d$ is used to represent the depth of the solution.
      \end{itemize}

    \subsection{Generic Search}
      Given a graph, generic search algorithms repeatedly pick a node and expand a search to its children. Different algorithms
      differ in how they select the next node to consider. Many algorithms maintain some queue-like structure for storing nodes
      on a fringe to be expanded.

    \subsection{Uninformed vs Informed Search}
      Uninformed search algorithms do not have any idea of the direction of the goal node, any simply expand nodes until they
      reach the goal. Informed search algorithms apply some domain knowledge to determine the general direction of goal nodes.
      These informed algorithms are also known as ``heuristic search'' algorithms.

    \subsection{Breadth-first Search}
      Breadth-first search (BFS) operates by expanding the shallowest unexpanded nodes, storing the fringe to be expanded in a FIFO
      queue.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete
          \item Optimal if cost is equal to depth, otherwise may be non-optimal
          \item Time complexity $O(b^{d+1})$
          \item Space complexity $O(b^{d+1})$
        \end{itemize}

    \subsection{Uniform Cost Search}
      Uniform cost search operates similarly to BFS, but instead of expanding the shallowest unexpanded node, expands the lowest cost
      unexpanded node. Note that BFS is a variant of uniform cost search where all edges have uniform cost.

      \subsubsection{Properties}
        Let $C$ be the cost of shortest path to the goal node, and other actions have a minimum cost of $\epsilon$.
        \begin{itemize}
          \item Complete
          \item Optimal
          \item Time complexity $O(b^{\frac{C}{\epsilon} + 1})$
          \item Space complexity $O(b^{\frac{C}{\epsilon} + 1})$
        \end{itemize}

    \subsection{Depth-first Search}
      Depth-first search (DFS) operates by expanding the deepest unexpanded nodes, storing nodes to be expanded in a LIFO stack.

      \subsubsection{Properties}
        \begin{itemize}
          \item May be incomplete in infinite-depth search spaces, or in spaces with loops
          \item Non-optimal
          \item Time complexity $O(b^m)$, note that $m$ may be much larger than $d$.
          \item Space complexity $O(bm)$
        \end{itemize}

    \subsection{Depth-limited Search}
      Depth-limited search operates like DFS but imposes a maximum depth on the search.

      \subsubsection{Properties}
        Let $l$ be the maximum depth of the search.
        \begin{itemize}
          \item Complete if there is a solution with depth $d \leq m$.
          \item Non-optimal
          \item Time complexity $O(b^l)$
          \item Space complexity $O(bl)$
        \end{itemize}

    \subsection{Iterative Deepening Search}
      Iterative deepening search operates by repeating a depth-limited search with a larger and larger
      search depth $l$ until a solution if found.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete if $b$ is finite.
          \item Guaranteed to return shallowest goal (similar to BFS), so optimal if cost is equal to depth
          \item Time complexity $O(b^d)$, note that $m$ may be much larger than $d$.
          \item Space complexity $O(bd)$
        \end{itemize}

  \section{Informed Search Strategies}
    Informed search algorithms apply domain knowledge in a problem to search the ``most promising'' branches first. This can lead to
    finding solutions more quickly, or finding solutions with lower costs than uninformed search algorithms.

    \subsection{Heuristics}
      The domain knowledge used in the problem is applied in the form of a heuristic function, which gives an estimate to the distance
      from the goal. Heuristics are often applied as a ``commonsense'' technique without many theoretical guarantees.

      A heuristic function $h(n)$ can be used to estimate the ``goodness'' of node n. In general:
      \begin{itemize}
        \item $h(n) \geq 0 \forall n$
        \item $h(n) = 0$ means n is a goal node.
        \item $h(n) = \infty$ means n is a dead end that does not lead to a goal.
      \end{itemize}

      A heuristic function is considered admissible (or optimistic) if it never overestimates the cost of reaching the goal.
      Admissible heuristics can often be derived by relaxing the rules to a problem.

    \subsection{Strong vs Weak Methods}
      Strong methods are designed a address a particular type of problem, while weak methods are general approaches that can be applied
      to many types of problems. Because of the limited way in which domain-specific information is used to solve a problem with heuristic
      search, it is known as a weak method, since a strong method would apply more powerful domain-specific heuristics. Other examples of
      weak methods include:
      \begin{itemize}
        \item \textbf{Means-end analysis} - A strategy where a representation is formed for the current and goal state, and actions are
        analyzed that shrink the difference between the two.
        \item \textbf{Space splitting} - A strategy where possible solutions to a problem are listed, and then classes of these solutions
        are ruled out to shrink the search space.
        \item \textbf{Subgoaling} - A strategy where a large problem is split into independent smaller ones.
      \end{itemize}

    \subsection{Best-first Search}
      Best-first search operates similarly to uniform cost search, but places nodes in a priority queue such that nodes in the queue are
      ordered by their values of $h(n)$.

      \subsubsection{Properties}
        \begin{itemize}
          \item Not complete - can get stuck in loops
          \item Non-optimal
          \item Time complexity $O(b^m)$, but good heuristic can yield major improvements
          \item Space complexity $O(b^m)$
        \end{itemize}

    \subsection{Beam Search}
      Beam search is an optimization of best-first search that reduces its memory requirement. Instead of expanding all nodes on a fringe,
      beam search only expands the $\beta$ (beam width) with the lowest values of $h(n)$.

      \subsubsection{Properties}
        \begin{itemize}
          \item Not complete
          \item Non-optimal
          \item Time complexity $O(\beta b)$
          \item Space complexity $O(\beta b)$
          \item Not admissible
        \end{itemize}

    \subsection{A* Search}
      A function $g(n) = g(n) + h(n)$ can be used to give the total estimated cost of a solution, where $g(n)$ is the cost from the start to n,
      and $h(n)$ is the heuristic function for node n. However, an algorithm based on this is not complete if $h(n)$ can be infinite, and is
      not admissible.

      A* search is a variation on the above algorithm with the constraint $h(n) \leq h^*(n)$, where $h^*(n)$ is the actual minimal path cost from
      n to a goal. This constraint makes A* search complete whenever the branching factor is finite and there is a fixed positive cost, and also
      makes it admissible.

      Some properties of A* search include:
      \begin{itemize}
        \item \textbf{Perfect heuristic:} If $h(n) = h^*(n)$, then only nodes on the optimal solution are expanded.
        \item \textbf{Null heuristic:} If $h(n) = 0$, then A* acts like uniform cost search
        \item \textbf{Better heuristic:} If $h_1(n) < h_2(n) < h^*(n)$, then $h_2$ is a better heuristic than $h_1$.
      \end{itemize}

    \subsection{Hill Climbing Search}
      Hill climbing search attempts to improve the efficiency of depth-first search. After starting with an arbitrary solution, hill climbing
      search attempts to improve the solution by changing a single element at a time. This is done by sorting the successors of a node according
      to their heuristic values, and then adding them to the list to be expanded. These changes are made until no further improvements can be found.

      The rule applied in hill climbing search is that if there is a successor $s$ for node $n$ such that:
      \begin{itemize}
        \item $h(s) < h(n)$ and
        \item $h(s) \leq h(t)$ for all successors $t$ of $n$.
      \end{itemize}
      then advance from $n$ to $s$, otherwise halt at $n$.

      Hill climbing search functions like a beam search with $\beta = 1$.

  \section{Game Playing as Search}
    Games involve playing against an opponent, where search problems involve finding a good move, waiting for an opponent's response, and then
    repeating. Time is typically limited in each search.

    \subsection{Types of Games}
      \begin{itemize}
        \item \textbf{Perfect information:} Each player has complete information on the opponent's state and available choices (e.g chess).
        \item \textbf{Imperfect information:} Each player does not have complete information on the opponent's state and available choices (e.g. poker).
      \end{itemize}

    \subsection{Max Min Strategy}
      In situations with perfect information and two players, a game tree can be expanded that describes all possible moves of the player and opponent
      in the game.

      The minimax strategy is commonly used in zero sum games where the goal is to minimize the maximum losses that can occur. Given a game tree, an
      optimal strategy (assuming both players play optimally from each position) can be derived by examining the minimax strategy of each node. Each
      level in a game tree is assigned alternating MAX (player) and MIN (opponent) values, where MAX is the maximum value of its successors and MIN is the
      minimum value of its successors.

      \subsubsection{Limiting Depth}
        In practice, game trees can only be fully expanded for simple games. For more complicated games, a limited depth of the game tree should be explored.
        In such circumstances, an evaluation function $f(n)$ is used to measure the ``goodness'' of a game state.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete if tree is finite
          \item Optimal (when playing against optimal opponent)
          \item Time complexity $O(b^d)$
          \item Space complexity $O(bd)$ (depth-first exploration)
        \end{itemize}

    \subsection{$\alpha$-$\beta$ Pruning}
      Alpha-beta pruning seeks to reduce the number of nodes that need to be generated and evaluated. The key idea behind this algorithm is to avoid
      processing subtrees that cannot have an effect on the result. Two new parameters are introduced, namely $\alpha$, which is the best value of MAX
      found so far, and $\beta$, which if the best value of MIN seen so far. Alpha is used in MIN nodes, and is assigned in MAX nodes, and vice versa for
      beta. The two introduced bounds are:
      \begin{itemize}
        \item \textbf{Alpha cutoff:} When value of min position is less than or equal to the alpha-value of its parent, stop generating further successors.
        \item \textbf{Beta cutoff:} When value of max position is greater than the beta-value of its parent, stop generating further successors.
      \end{itemize}

      The output of alpha-beta pruning is exactly the same as the output of the minimax algorithm; alpha-beta pruning is purely an efficiency consideration.
      The time complexity of the minimax algorithm is $O(b^d)$, while the time complexity of alpha-beta pruning is between $O(b^\frac{d}{2})$ and $O(b^d)$
      depending on the properties of the search tree.

  \section{Metaheuristics}
    Metaheuristics are high-level heuristics designed to select other heuristics to solve a problem. Search algorithms making use of metaheuristics are approximate
    and usually non-deterministic, and vary from simple local search to complex learning processes.

    \subsection{Common Properties}
      \begin{itemize}
        \item Mechanisms to avoid getting trapped in confined areas of search space.
        \item Not being problem-specific, though may make use of domain-specific knowledge from heuristics controlled by upper-level strategy.
        \item Search experience (i.e. some representation of search history) to guide the search.
        \item Hybrid search models where the search identifies neighborhoods where a goal may lie, and then the search is intensified in that area.
      \end{itemize}

    \subsection{Population-based Methods}
      Population-based methods are metaheuristic approaches that apply multiple agents to a search space and can handle multiple simultaneous solutions.

    \subsection{Trajectory Methods}
      Trajectory methods are metaheuristic variants of local search that apply memory structure to avoid getting stuck at local minima, and implement an explorative
      strategy that tries to avoid revisiting nodes.

\end{document}
