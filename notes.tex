\documentclass[12pt,titlepage]{article}
\usepackage[margin=1in]{geometry}
\usepackage{parskip}

\usepackage{hyperref}
\hypersetup{
  linktoc=all
}

\begin{document}
  \begin{titlepage}
    \vspace*{\fill}
    \centering

    \textbf{\Huge ECE 457A Course Notes} \\ [0.4em]
    \textbf{\Large Cooperative and Adaptive Algorithms} \\ [1em]
    \textbf{\Large Michael Socha} \\ [1em]
    \textbf{\large 4A Software Engineering} \\
    \textbf{\large University of Waterloo} \\
    \textbf{\large Spring 2018} \\
    \vspace*{\fill}
  \end{titlepage}

  \newpage 

  \tableofcontents

  \newpage

  \section{Course Overview}
    \subsection{Logistics}
      \begin{itemize}
        \item \textbf{Professor:} Allaa Hilal
      \end{itemize}

    \subsection{Overview of Topics}
      This is a course that approaches artificial intelligence by examining various algorithms that adapt to their
      environment and cooperate with one another. Topics include:
      \begin{itemize}
        \item Algorithms that exhibit intelligent behavior
        \item Concepts of adaptation and cooperation between algorithms
        \item Meta-heuristics, evolutionary computing methods, swarm intelligence, ant-colony algorithms, and particle
        swarm methods
        \item Apply the above ideas to solve various (often ill-structured) continuous and discrete problems
      \end{itemize}

  \newpage

  \section{Introduction}

    \subsection{What is Artificial Intelligence (AI)?}
      Intelligence is the ability to acquire and apply knowledge and skills. AI is the science of creating intelligent machines,
      including intelligent computer programs. Sample applications include visual perception, speech recognition and various
      forms of decision-making

      \subsubsection{AI vs Machine Learning vs Deep Learning vs Data Science}
        The above terms are often thrown around almost interchangealbly, but they refer to different things. Deep learning is
        a type of machine learning, which is a type of AI. Data science is a separate field that has some overlap with these
        three other concepts.

      \subsubsection{Inspiration from Nature}
        Some of the inspirations for intelligent systems come from the natural world. Sample inspirations include ant path-finding,
        bird flocking, and fish schooling.

    \subsection{Thinking Rationally vs Behaving Rationally}
      In rational thinking, logical systems are used to achieve goals via inferencing. It can be hard to represent informal knowledge,
      and not all problems can be solved through such methods (e.g. problems with uncertainty).

      In rational behavior, a perceives its environment and acts to achieve goals according to some set of beliefs. This is a more
      general approach than inferencing, and actions taken to achieve a goal may not necessarily be optimal or ``correct'', but
      may be an acceptable solution anyways.
      
    \subsection{Swarm Intelligence}
      Swarm intelligence is an AI technique that builds systems based on the collective behavior of decentralized, self-organized units.
      There are no centralized control structures, with agents only interacting with each other and the environment.

    \subsection{Intelligent Agents}
      An agent is something that senses its environment and acts on the collected information. A rational agent acts in a way that is
      expected to maximize performance on the basis of perceptual history and built-in knowledge. Types of agents include:
      \begin{itemize}
        \item \textbf{Simple Reflex Agents:} Follow a lookup-table approach; needs fully observable environment
        \item \textbf{Model-Based Reflex Agents:} Add state information to handle partially observable environments
        \item \textbf{Goal-Based Agents:} Add concept of goals to help choose actions
        \item \textbf{Utility-Based Agents:} Add utility to decide ``good'' or ``'bad'' when faced with conflicting goals
        \item \textbf{Learning Agents:} Add ability to learn from experience to improve performance
      \end{itemize}

    \subsection{Environments}
      The environments in which agents operate influence the design of the agents. Types of environments include:
      \begin{itemize}
        \item \textbf{Fully vs Partially Observable:} In fully observable environments, sensors can detect all aspects
        relevant to choice of action. This is not the case in partially observable environments, which may exist because
        of missing information or inaccurate sensors.
        \item \textbf{Deterministic vs Stochastic:} Deterministic environments are only influenced by their current
        state and the next action executed by the agent; otherwise, the environment is stochastic.
        \item \textbf{Episodic vs Sequential:} In episodic environments, the choice of action in each episode does not
        depend on previous episodes, unlike in sequential environments, where an agent needs to ``think ahead''.
        \item \textbf{Static vs Dynamic:} Environments where the state may change while the agent is deliberating are
        considered dynamic; other environments are considered static.
        \item \textbf{Discrete vs Continuous:} A discrete/continuous distinction can be applied to various aspects of
        an environment, including the way time flow is handled and to the actions of the agent.
        \item \textbf{Single vs Multi Agent}
      \end{itemize}

    \subsection{Adaptive and Cooperative Algorithms}
      Adaptive algorithms are able to change their behavior as they are run. Cooperative algorithms algorithms work
      together to solve a joint problem, communicating either directly or indirectly with one another.

  \newpage

  \section{Search Problem Formulation}
    The first step in solving a search problem lies in formulation. Sources of potential complexity in formalizing and
    solving real-world search problems include multimodality, lack of a mathematical model, non-differentiability, and
    a combinatoric or distributed nature.

    \subsection{Well-Structured vs Ill-Structured Problems}
      In well-structured problems, the existing and end state are well-defined, and so are the actions that can be taken
      to change state. In ill-structured problems, the existing state is well-defined, but the end state and actions to
      change state are typically undefined.

    \subsection{Optimization Problems}
      Optimization problems concern finding the ``best'' solution from a set of solutions subject to a set of constraints.
      Optimization techniques are search methods where the goal is to find a solution to an optimization problem. Optimization
      techniques can be divided into exact algorithms, where an optimal solution is found but often at a very high computational
      cost, and approximate solutions, where a near-optimal solution is found at a far lower computational cost.

      \subsubsection{Approximation Methods}
        Due to their relatively low computational costs, approximation methods are often preferred for solving problems of a
        combinatorial nature where heuristics can be applied to find near-optimal solutions. Approaches to approximation can
        be divided into constructive methods, where a solution is built up one component at a time, and local search methods,
        where actions are iteratively applied to some initial solution to attempt to improve it.

    \subsection{Goal and Problem Formulation}
      In goal formulation, decisions are made on which aspects of a problem are important and which can be ignored. Problem
      formulation concerns determining how to manipulate these important aspects. Goal formulation followed by problem
      formulation yields the following information:
      \begin{itemize}
        \item \textbf{State space:} Complete or partial configuration of a problem
        \item \textbf{Initial state:} State in which search begins
        \item \textbf{Goal state:} State in which search ends
        \item \textbf{Action set:} Set of possible transitions between states
        \item \textbf{Cost:} Can be used to compare various solutions
      \end{itemize}

      Generally speaking, a closed-world assumption can be followed, meaning that each state is a complete description of the world.
      Environments in closed-world problems are typically fully observable, deterministic, sequential, static, and discrete.

    \subsection{Properties of Search Algorithms}
      The key properties of search algorithms are:
      \begin{itemize}
        \item \textbf{Completeness:} Whether algorithm is guaranteed to find goal node, provided one exists
        \item \textbf{Optimality:} Whether algorithm is guaranteed to find the best goal node (i.e. one with cheapest cost)
        \item \textbf{Time complexity:} How many nodes generated
        \item \textbf{Space complexity:} Maximum number of nodes stored in memory
      \end{itemize}

  \newpage

  \section{Graph Search Algorithms}

    \subsection{Search Tree Terminology}
      \begin{itemize}
        \item \textbf{Node:} Represents a state in a search problem
        \item \textbf{Edge:} Represents a transition between states
        \item \textbf{Branching factor (b):} Maximum number of child nodes extending from a parent node
        \item \textbf{Maximum depth (m):} Number edges in the shortest path from the root node to the furthest node.
        Variable $d$ is used to represent the depth of the solution.
      \end{itemize}

    \subsection{Generic Search}
      Given a graph, generic search algorithms repeatedly pick a node and expand a search to its children. Different algorithms
      differ in how they select the next node to consider. Many algorithms maintain some queue-like structure for storing nodes
      on a fringe to be expanded.

    \subsection{Uninformed vs Informed Search}
      Uninformed search algorithms do not have any idea of the direction of the goal node, and simply expand nodes until they
      reach the goal. Informed search algorithms apply some domain knowledge to determine the general direction of goal nodes.
      These informed algorithms are also known as ``heuristic search'' algorithms.

    \subsection{Breadth-first Search}
      Breadth-first search (BFS) operates by expanding the shallowest unexpanded nodes, storing the fringe to be expanded in a FIFO
      queue.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete
          \item Optimal if cost is equal to depth, otherwise may be non-optimal
          \item Time complexity $O(b^{d+1})$
          \item Space complexity $O(b^{d+1})$
        \end{itemize}

    \subsection{Uniform Cost Search}
      Uniform cost search operates similarly to BFS, but instead of expanding the shallowest unexpanded node, expands the lowest cost
      unexpanded node. Note that BFS is a variant of uniform cost search where all edges have uniform cost.

      \subsubsection{Properties}
        Let $C$ be the cost of shortest path to the goal node, and other actions have a minimum cost of $\epsilon$.
        \begin{itemize}
          \item Complete
          \item Optimal
          \item Time complexity $O(b^{\frac{C}{\epsilon} + 1})$
          \item Space complexity $O(b^{\frac{C}{\epsilon} + 1})$
        \end{itemize}

    \subsection{Depth-first Search}
      Depth-first search (DFS) operates by expanding the deepest unexpanded nodes, storing nodes to be expanded in a LIFO stack.

      \subsubsection{Properties}
        \begin{itemize}
          \item May be incomplete in infinite-depth search spaces, or in spaces with loops
          \item Non-optimal
          \item Time complexity $O(b^m)$, note that $m$ may be much larger than $d$.
          \item Space complexity $O(bm)$
        \end{itemize}

    \subsection{Depth-limited Search}
      Depth-limited search operates like DFS but imposes a maximum depth on the search.

      \subsubsection{Properties}
        Let $l$ be the maximum depth of the search.
        \begin{itemize}
          \item Complete if there is a solution with depth $d \leq m$.
          \item Non-optimal
          \item Time complexity $O(b^l)$
          \item Space complexity $O(bl)$
        \end{itemize}

    \subsection{Iterative Deepening Search}
      Iterative deepening search operates by repeating a depth-limited search with a larger and larger
      search depth $l$ until a solution if found.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete if $b$ is finite.
          \item Guaranteed to return shallowest goal (similar to BFS), so optimal if cost is equal to depth
          \item Time complexity $O(b^d)$, note that $m$ may be much larger than $d$.
          \item Space complexity $O(bd)$
        \end{itemize}

  \newpage

  \section{Informed Search Strategies}
    Informed search algorithms apply domain knowledge in a problem to search the ``most promising'' branches first. This can lead to
    finding solutions more quickly, or finding solutions with lower costs than uninformed search algorithms.

    \subsection{Heuristics}
      The domain knowledge used in the problem is applied in the form of a heuristic function, which gives an estimate to the distance
      from the goal. Heuristics are often applied as a ``commonsense'' technique without many theoretical guarantees.

      A heuristic function $h(n)$ can be used to estimate the ``goodness'' of node n. In general:
      \begin{itemize}
        \item $\forall n, h(n) \geq 0$
        \item $h(n) = 0$ means n is a goal node.
        \item $h(n) = \infty$ means n is a dead end that does not lead to a goal.
      \end{itemize}

      A heuristic function is considered admissible (or optimistic) if it never overestimates the cost of reaching the goal.
      Admissible heuristics can often be derived by relaxing the rules to a problem.

    \subsection{Strong vs Weak Methods}
      Strong methods are designed a address a particular type of problem, while weak methods are general approaches that can be applied
      to many types of problems. Because of the limited way in which domain-specific information is used to solve a problem with heuristic
      search, it is known as a weak method, since a strong method would apply more powerful domain-specific heuristics. Other examples of
      weak methods include:
      \begin{itemize}
        \item \textbf{Means-end analysis} - A strategy where a representation is formed for the current and goal state, and actions are
        analyzed that shrink the difference between the two.
        \item \textbf{Space splitting} - A strategy where possible solutions to a problem are listed, and then classes of these solutions
        are ruled out to shrink the search space.
        \item \textbf{Subgoaling} - A strategy where a large problem is split into independent smaller ones.
      \end{itemize}

    \subsection{Best-first Search}
      Best-first search operates similarly to uniform cost search, but places nodes in a priority queue such that nodes in the queue are
      ordered by their values of $h(n)$.

      \subsubsection{Properties}
        \begin{itemize}
          \item Not complete - can get stuck in loops
          \item Non-optimal
          \item Time complexity $O(b^m)$, but good heuristic can yield major improvements
          \item Space complexity $O(b^m)$
        \end{itemize}

    \subsection{Beam Search}
      Beam search is an optimization of best-first search that reduces its memory requirement. Instead of expanding all nodes on a fringe,
      beam search only expands the $\beta$ (beam width) with the lowest values of $h(n)$.

      \subsubsection{Properties}
        \begin{itemize}
          \item Not complete
          \item Non-optimal
          \item Time complexity $O(\beta b)$
          \item Space complexity $O(\beta b)$
          \item Not admissible
        \end{itemize}

    \subsection{A* Search}
      A function $g(n) = g(n) + h(n)$ can be used to give the total estimated cost of a solution, where $g(n)$ is the cost from the start to n,
      and $h(n)$ is the heuristic function for node n. However, an algorithm based on this is not complete if $h(n)$ can be infinite, and is
      not admissible.

      A* search is a variation on the above algorithm with the constraint $h(n) \leq h^*(n)$, where $h^*(n)$ is the actual minimal path cost from
      n to a goal. This constraint makes A* search complete whenever the branching factor is finite and there is a fixed positive cost, and also
      makes it admissible.

      Some properties of A* search include:
      \begin{itemize}
        \item \textbf{Perfect heuristic:} If $h(n) = h^*(n)$, then only nodes on the optimal solution are expanded.
        \item \textbf{Null heuristic:} If $h(n) = 0$, then A* acts like uniform cost search
        \item \textbf{Better heuristic:} If $h_1(n) < h_2(n) < h^*(n)$, then $h_2$ is a better heuristic than $h_1$.
      \end{itemize}

    \subsection{Hill Climbing Search}
      Hill climbing search attempts to improve the efficiency of depth-first search. After starting with an arbitrary solution, hill climbing
      search attempts to improve the solution by changing a single element at a time. This is done by sorting the successors of a node according
      to their heuristic values, and then adding them to the list to be expanded. These changes are made until no further improvements can be found.

      The rule applied in hill climbing search is that if there is a successor $s$ for node $n$ such that:
      \begin{itemize}
        \item $h(s) < h(n)$ and
        \item $h(s) \leq h(t)$ for all successors $t$ of $n$.
      \end{itemize}
      then advance from $n$ to $s$, otherwise halt at $n$.

      Hill climbing search functions like a beam search with $\beta = 1$.

  \newpage

  \section{Game Playing as Search}
    Games involve playing against an opponent, where search problems involve finding a good move, waiting for an opponent's response, and then
    repeating. Time is typically limited in each search.

    \subsection{Types of Games}
      \begin{itemize}
        \item \textbf{Perfect information:} Each player has complete information on the opponent's state and available choices (e.g chess).
        \item \textbf{Imperfect information:} Each player does not have complete information on the opponent's state and available choices (e.g. poker).
      \end{itemize}

    \subsection{Max Min Strategy}
      In situations with perfect information and two players, a game tree can be expanded that describes all possible moves of the player and opponent
      in the game.

      The minimax strategy is commonly used in zero sum games where the goal is to minimize the maximum losses that can occur. Given a game tree, an
      optimal strategy (assuming both players play optimally from each position) can be derived by examining the minimax strategy of each node. Each
      level in a game tree is assigned alternating MAX (player) and MIN (opponent) values, where MAX is the maximum value of its successors and MIN is the
      minimum value of its successors.

      \subsubsection{Limiting Depth}
        In practice, game trees can only be fully expanded for simple games. For more complicated games, a limited depth of the game tree should be explored.
        In such circumstances, an evaluation function $f(n)$ is used to measure the ``goodness'' of a game state.

      \subsubsection{Properties}
        \begin{itemize}
          \item Complete if tree is finite
          \item Optimal (when playing against optimal opponent)
          \item Time complexity $O(b^d)$
          \item Space complexity $O(bd)$ (depth-first exploration)
        \end{itemize}

    \subsection{$\alpha$-$\beta$ Pruning}
      Alpha-beta pruning seeks to reduce the number of nodes that need to be generated and evaluated. The key idea behind this algorithm is to avoid
      processing subtrees that cannot have an effect on the result. Two new parameters are introduced, namely $\alpha$, which is the best value of MAX
      found so far, and $\beta$, which if the best value of MIN seen so far. Alpha is used in MIN nodes, and is assigned in MAX nodes, and vice versa for
      beta. The two introduced bounds are:
      \begin{itemize}
        \item \textbf{Alpha cutoff:} When value of min position is less than or equal to the alpha-value of its parent, stop generating further successors.
        \item \textbf{Beta cutoff:} When value of max position is greater than the beta-value of its parent, stop generating further successors.
      \end{itemize}

      The output of alpha-beta pruning is exactly the same as the output of the minimax algorithm; alpha-beta pruning is purely an efficiency consideration.
      The time complexity of the minimax algorithm is $O(b^d)$, while the time complexity of alpha-beta pruning is between $O(b^\frac{d}{2})$ and $O(b^d)$
      depending on the properties of the search tree.

  \newpage

  \section{Metaheuristics}
    Metaheuristics are high-level heuristics designed to select other heuristics to solve a problem. Search algorithms making use of metaheuristics are approximate
    and usually non-deterministic, and vary from simple local search to complex learning processes.

    \subsection{Common Properties}
      \begin{itemize}
        \item Mechanisms to avoid getting trapped in confined areas of search space.
        \item Not being problem-specific, though may make use of domain-specific knowledge from heuristics controlled by upper-level strategy.
        \item Search experience (i.e. some representation of search history) to guide the search.
        \item Hybrid search models where the search identifies neighborhoods where a goal may lie, and then the search is intensified in that area.
      \end{itemize}

    \subsection{Population-based Methods}
      Population-based methods are metaheuristic approaches that apply multiple agents to a search space and can handle multiple simultaneous solutions.

    \subsection{Trajectory Methods}
      Trajectory methods are metaheuristic variants of local search that apply memory structure to avoid getting stuck at local minima, and implement an explorative
      strategy that tries to avoid revisiting nodes.

  \newpage

  \section{Tabu Search (TS)}
    TS is a widely used, general trajectory-based metaheuristic strategy for controlling inner heuristics. TS can be seen as a combination of a local search strategy with a search experience
    model that is used to escape local minima and implement and explorative strategy.

    \subsection{Local Search}
      Local search strategies start with an initial feasible solution, after which a series of local modifications are applied. If a modification generates a better solution,
      then the process is repeated from that node. Two major challenges faced by local search algorithms include:
      \begin{itemize}
        \item Can be costly to consider all possible local modifications. 
        \item Can get stuck in local optimum.
      \end{itemize}

    \subsection{Basic Ideas}
      TS attempts to overcome the limitations of local search alone. A key idea behind this is to penalize moves that take a solution back to a previously visited (tabu) state.
      This can result in non-improving solutions being accepted in order to escape from local optima and eventually find a better solution.

    \subsection{Use of Memory}
      TS applies ``short-term'' memory structures to store the recency of occurrence of a previous solution to. This is often known as a Tabu list, where the Tabu list length
      (or Tabu tenure) refers to the number of iterations for which moves are kept. Tabu lists rarely hold complete solutions due to their large memory requirements, and often
      instead store some related data (e.g. moves used to generate past solutions). When selecting a new neighborhood to explore, neighborhoods outside of the Tabu list are preferred.

      TS can also apply ``long-term'' memory structures to store the frequency of solution components, which can also be used to diversify the search by exploring unvisited
      areas of the solution space.

    \subsection{Termination Conditions}
      Sample TS termination conditions include:
      \begin{itemize}
        \item No feasible solution in neighborhood of current solution.
        \item Maximum number of iterations reached.
        \item Number of iterations since last improvement has crossed an upper bound.
        \item Some sort of evidence shows that an optimal solution has been obtained.
      \end{itemize}

    \subsection{Candidates and Aspiration}
      A candidate list stores the potential solutions in a neighborhood to be examined. This is done by isolating regions of a neighborhood with desirable features, aiming
      to find a solution more efficiently than by just searching all possible solutions in a neighborhood. At times, it can be desirable to include a move in a candidate list
      even if it is tabu in order to prevent stagnation. Approaches used to cancel tabus are referred to as aspiration criteria. 

    \subsection{Selecting Tabu Restrictions}
      Examples of restrictions for picking the next move include:
      \begin{itemize}
        \item Not picking the same exchange of positions as in a tabu move.
        \item Not picking a move that results in a positions that previously appeared in a tabu move.
      \end{itemize}

    \subsection{Selecting Tabu Tenure}
      Tabu tenure $T$ can be selected in the following ways:
      \begin{itemize}
        \item Statically assigning $T$ to be a constant (as a general rule, $\sqrt{n}$ where $n$ is the problem size.
        \item Dynamically letting $T$ vary between a $T_{min}$ and $T_{max}$. Dynamic Tabu tenures tend to be better at limiting cycling,
        though some searches may contain cycles longer than the Tabu tenure.
      \end{itemize}

    \subsection{Selecting Aspiration Criteria}
      Aspiration criteria can be selected based on the following strategies:
      \begin{itemize}
        \item By default, where a tabu move becomes admissible if it yields a better solution than any found so far.
        \item By objective, where a tabu move becomes admissible if it yield a solution better than an aspiration value.
        \item By search direction, where a tabu move becomes admissible if the direction of the search remains constant (e.g. non-improving remains non-improving).
      \end{itemize}

    \subsection{Intensification}
      Intensification refers to the process of exploiting a small portion of the search space (e.g. penalizing solutions far from current solution). Intensification
      can be used to locally optimize a best known solution while trying to preserve the general components of that solution (based on short-term memory).

    \subsection{Diversification}
      Diversification refers to the process of forcing the search into unexplored areas (e.g. penalizing solutions close to current solution). While Tabu lists can aid
      in diversification, they often act very locally, and long-term memory known as the ``frequency memory'' is used to store the number of iterations in which certain
      components always appear in solutions. Diversification strategies can include:
      \begin{itemize}
        \item \textbf{Restart diversification}, where components rarely appearing in solutions are forced into new solutions.
        \item \textbf{Continuous diversification}, where the evaluation of possible moves is biased by a term related to component frequency.
      \end{itemize}

    \subsection{Adaptation}
      Adaptation refers to a series of techniques for varying the Tabu tenure. As a general rule, if the Tabu tenure is too small, then cycles in the search are likely,
      while if it is too big, then many moves could be prevented at each iteration.

      Several difference techniques can be applied to vary to Tabu tenure. One such technique involves randomly selecting a new tenure from a pre-computed range every
      predetermined number of iterations. Another approach involves setting to tenure to 1 if a best-so-far solution is found, lowering it in an improving phase, and
      increasing it in a worsening phase. This can be coupled by randomly changing the min and max values of the tenures for each fixed number of moves.

    \subsection{Cooperation}
      Cooperation can be applied by multiple Tabu searches running concurrently. One such approach involves coordinated searches exchanging information every fixed number
      of iterations (synchronous communication), where an incoming solution to a search agent can be handled by replacing its own best-so-far (forced diversification),
      or by replacing its own best-so-far solution only if the incoming solution is better (conditional import). Alternatively, asynchronous communication can be applied
      where search agents relay their best-so-far results to a central memory. If this best-so-far result if better than the global one, then it replaces the global one.
      Otherwise, search agents continue using their worse one for a certain number of moves, after which they fetch the solution stored in the central memory. Some variants
      store multiple solutions in central memory, where a random one is assigned to agents requesting a solution.

      General results from cooperation experiments found that:
      \begin{itemize}
        \item Increasing the number of search agents improves the solution up to a certain point.
        \item Increasing the number of synchronization messages increases the computation time due to message passing overhead.
        \item Conditional imports are almost always preferable to forced diversification.
      \end{itemize}

  \newpage

  \section{Simulated Annealing}

    \subsection{Physical Annealing Analogy}
      Physical annealing involves heating a substance (e.g. a metal) and then letting it cool to increase its ductility and reduce hardness. The goal is to make the molecules
      in a cooled substances arrange themselves in a low-energy structure, and the properties of this structure are influenced by the temperatures reached and the rate of cooling.
      A sequence of cooling times and temperatures is referred to as an annealing or cooling schedule.

      Simulated annealing is a search algorithm that mimics the physical annealing process. First introduced in 1953, simulated annealing leads to a random walk between
      solutions that, under certain conditions, is guaranteed to sample from the probability distributions of an ideal equilibrium state. Further contributions were made in 1983
      and 1985 that applied simulated annealing to finding approximations of global optima in large search spaces.

    \subsection{Basics of Simulated Annealing}
      Simulated annealing search algorithms start with some arbitrary initial solution, ``temperature'', and temperature reduction function. Then until some sort of stopping condition
      is satisfied, the algorithm repeatedly picks new potential solutions from a neighborhood $N(s)$. If the solution has a lower cost than the previously found one, then it is accepted.
      Otherwise, the solution is accepted only if a random variable $x$ in the range $(0, 1)$ is lesser than $e^{- \frac{\Delta c}{t}}$, where $\Delta c$ is the increase in cost and $t$ is
      the temperature. After each step, the temperature reduction function is applied.

    \subsection{Strategy}
      The general strategy of simulated annealing is to encourage exploring more of the search space at the beginning when the temperature is high. Later on, once the algorithm converges
      on an approximation of the global optimum, accepting worse solutions to facilitate exploration becomes less probable. Lower temperatures as well as higher changes in cost lower the
      probability of a particular change being accepted as a new solution.

    \subsection{Annealing Schedule}
      An annealing schedule provides a mapping from time to temperature, and is determined by a search's initial temperature, final temperature, rules for decrementing temperatures, and the
      number of search iterations at each temperature.

      \subsubsection{Initial Temperature}
        The initial temperature should be high enough to allow exploration to any part of the search space. However, making the initial temperature too hot will make the search behave almost
        completely randomly. The maximum change of a cost function should be considered when setting the initial temperature, which as a general rule, should accept around 60\% of worse
        solutions.

      \subsubsection{Final Temperature}
        A final temperature should be quite low but does not necessarily have to reach 0. A search using simulated annealing can be stopped once no better moves are being found and no
        worse moves are being accepted.

      \subsubsection{Temperature Decrement Rules}
        Two basic techniques for decrementing temperature are though a linear ($t = t - \alpha$) and a geometric ($t = t \cdot \alpha$) function. For geometric decrement, the recommended
        value of $\alpha$ is between 0.8 and 0.99. Alternatively, an technique that facilitates slow decrease ($t = \frac{t}{1 - \beta t}$) can be applied.

      \subsubsection{Temperature Iterations}
        Ideally, enough iterations should be performed for a search to become stable at each tested temperature. Large population sizes are a common reason for increasing this number
        of iterations, while slowly decreasing temperatures are a common reason for decreasing the number of iterations. A constant value of iterations is commonly used, though
        it is common to increase the number of iterations as the search increases to fully explore local optima.

    \subsection{Convergence of Simulated Annealing}
      Simulated annealing is guaranteed to eventually converge to a solution at a constant temperature, assuming some sequence of moves leads to the goal state. This becomes much
      more nuanced when temperature is not constant. where convergence can still be guaranteed but only under conditions that result in very slow temperature reduction and an
      exponential increase in the number of iterations at each temperature. Hence, this convergence theory is not commonly used in practice, though it is worth noting that
      simulated annealing is better backed theoretically than many other heuristic methods.

    \subsection{Adaptation}
      Adaptation in simulated annealing refers to adapting the critical parameters of the algorithm, including the initial temperature, cooling schedule, and number of iterations.

      \subsubsection{Initial Temperature}
        Finding the right temperature is very problem-specific, and different search algorithms can be applied in finding this temperature.

      \subsubsection{Cooling Schedule}
        Some cooling schedules require that only the cooling rate $\alpha$ is specified, and the remainder of parameters are automatically determined using a linear random combination
        of previously accepted states and parameters to estimate new steps and parameters.

      \subsubsection{Probability of Acceptance}
        Some attempted adaptations concerning the probability of acceptance include using a lookup table for the relevant calculations (to decrease computation time) or using a different,
        non-exponential probability formula.

      \subsubsection{Cost Function}
        Cost functions that return similar values for many different states tend to not lead to effective search. As an alternative, a cost function can have a penalty term associated with
        certain types of states, and the weighting of these penalty terms can vary dynamically.

    \subsection{Cooperation}
      Cooperative simulated annealing involves multiple concurrent runs of a simulated annealing search algorithm on a search space. Potential solutions are produced by somehow combining
      the value of a run with the value of a random previous run of the algorithm. This exchange of information from other solutions is known as cooperative transition, and is a concept
      borrowed from genetic algorithms.

  \newpage

  \section{Evolutionary Algorithms}
    Evolutionary computing forms a category of population-based meta-heuristic methods whose behavior is inspired by biological evolution. These algorithms feature a population of
    individuals competing for limited resources, with the ``fitter'' individuals being used as seeds to form future generations. Over time, this population rises in overall fitness
    due to the principles of natural selection. Evolutionary algorithms are stochastic, with variation operators (crossover and mutation) propagating changes to new generations.

    \subsection{Active Information}
      Unless a search algorithm takes advantage of problem-specific information about a search target of search space, its average performance is that of random search. Three measures of
      information that can increase the effectiveness of a search can be categorized as:
      \begin{itemize}
        \item \textbf{Endogeneous information}, which measures the difficulty of finding a target through random search.
        \item \textbf{Exogeneous information}, which measures the difficulty of finding a target once problem-specific information is applied.
        \item \textbf{Active information}, which is the difference between exogenous and endogeneous information (i.e. measures the contribution of problem-specific information in solving
          a problem).
      \end{itemize}

  \newpage

  \section{Genetic Algorithms}
    Genetic algorithms are a class of evolutionary algorithms that operate by maintaining a population of candidate solutions and iteratively applying a set of stochastic operators,
    namely selection, reproduction, and mutation. These algorithms are inspired by Darwin's theory of natural selection, with the population eventually moving towards fitter solutions.

    \subsection{Simple Genetic Algorithms (SGAs)}
      The idea of genetic algorithms was first introduced by John Henry Hollard in 1975. The first described genetic algorithms are now known as simple genetic algorithms (SGAs), also
      known as classical or canonical SGAs.

      SGAs start by initializing a population with random candidate solutions. Then, parents are selected, after which a new generation is formed through the genetic operations of
      recombination and mutation. This generation replaces the parent generation, and the breeding cycle repeats until some termination criteria is reached. Example termination
      criteria include the search going passed a certain number of iterations, no improvement in the solution for a certain number of iterations, or the fitness of the solution
      passing a certain threshold.

      \subsubsection{Representation}
        SGAs represent candidate solutions as binary strings, which are often referred to as chromosomes of genotypes. Each binary digit is considered to be a separate gene, and
        the ordering of the genes can have important implications. This, the mapping from a problem's parameter domain to this binary representation is one of the most important
        factors for the performance of an SGA algorithm.

        Binary representations of a problem often apply gray coding, which means that small differences in the underlying solution result in small changes in the binary
        representation. Some implementations use representations other than binary strings, including integers or floating-point numbers.

      \subsubsection{Selection}
        Individuals are selected to be parents in the future generation with a probability proportional to their fitness (i.e. proportional selection). Based on this principle,
        $N$ parents can be selected from a population of $N$ solutions. The key idea behind this selection is to facilitate an increase in the fitness of future generations while
        also maintaining an element of randomness.

        A major limitation of fitness-proportional selection (FPS) is that it does not provide guarantees on the distribution of selected parents, since each selection is done
        independently. An alternative is stochastic universal sampling, which can be imagined as spinning a roulette wheel with $n$ evenly spaced arms to find $n$ values.

        Some other common problems with FPS algorithms are premature convergence (e.g. one highly fit member dominating a population), and a lack of selection pressure at the end
        of runs with similar fitness. Some attempts to overcome these problems include selection proportional to rank rather than absolute fitness, and tournament selection where
        random subsets of individuals are found, and the fittest members of these subsets are selected.

      \subsubsection{Crossover}
        Crossover/recombination is applied between two parents with a probability of $P_c$, which is typically in the range (0.6, 0.9). If no crossover takes place, the two parents
        are copied to two offspring unmodified. The following types of crossovers can be applied:
        \begin{itemize}
          \item \textbf{1-point crossover}, where a random point is chosen on the two parents, and the two children are formed by exchanging the tails this point divides.
          \item \textbf{n-point crossover}, which is a generalization of 1-point crossovers where n points are chosen on the two parents.
          \item \textbf{Uniform crossover}, where each gene has an independent $\frac{1}{2}$ chance of undergoing recombination, which makes inheritance independent of position.
        \end{itemize}

      \subsubsection{Mutation}
        After recombination, each gene can be altered with a probability of $p_m$. $p_m$ is typically between $\frac{1}{popsize}$ and $\frac{1}{chromosomelength}$. Crossovers
        tends to result in large changes in a population, while mutation results in small ones. Thus, crossovers are considered to be explorative, making a big jump to a
        possibly unexplored area between the two parent solutions, while mutations are exploitative, introducing small amounts of new information to further explore near an
        existing solution.

      \subsubsection{Population Models}
        SGA typically applies a generational GA model (GGA), where individuals survive for exactly one generation before they are replaced by offspring. An alternative is a
        steady-state GA model (SSGA), where part of a population is replaced by offspring. The proportion of the population replaced between successive generations is known as a
        generational gap ($1$ for GGA, and typically $\frac{1}{popsize}$ for SSGA).

        The process of selecting individuals from parents and offspring to make up the next generation is known as survivor selection. Common approaches for survivor selection
        include age-based (e.g. delete oldest) and fitness-based (e.g. keep best, delete worst) selection.

    \subsection{Real-Valued Genetic Algorithms}
      It is impractical to map many problems with real-values parameters to binary representations. Although real-valued GAs are similar to the SGAs discussed above they can
      differ as described below:

      \subsubsection{Crossover}
        It is possible to still apply discrete crossover techniques as done for binary chromosomes. However, real-valued chromosomes open up the possibility for the following
        intermediate crossovers:
        \begin{itemize}
          \item \textbf{Single arithmetic crossover}, where for a single parent gene pair $x$ and $y$, one child's gene becomes $\alpha x + (1 - \alpha) y$, and the reverse for the other child. 
          \item \textbf{Simple arithmetic crossover}, where for each parent gene pair $x$ and $y$, after a certain gene, one child's gene becomes $\alpha x + (1 - \alpha) y$, and the reverse
            for the other child. 
          \item \textbf{Whole arithmetic crossover}, where for each parent gene pair $x$ and $y$, one child's gene becomes $\alpha x + (1 - \alpha) y$, and the reverse for the other child.
        \end{itemize}

      \subsubsection{Mutation}
        With real-valued chromosomes, mutations can be applied by assigning a uniform random value between some lower and upper bound to a gene. Some variants on this technique also
        add some noise (e.g. from a Gaussian distribution) to this number.

    \subsection{Permutation Genetic Algorithms}
      Permutation problems are problems that deal with finding a sequence in which elements are arranged. There are two main classes of such problems:
      \begin{itemize}
        \item Problems that deal with adjacency of elements (e.g. Travelling Salesman Problem).
        \item Problems that deal with the overall order of elements (e.g. Job Shop Scheduling).
      \end{itemize}

      \subsubsection{Crossover}
        Previously defined crossover operations often result in inadmissible solutions (e.g. crossing [1,2,3] with [3,2,1] can result in duplicates of one element in an arrangement).

        Below are some crossover operations than can be applied to adjacency-based problems:
        \begin{itemize}
          \item \textbf{Partially Mapped Crossover (PMX)}, which follows the approach below to generate a child from parents P1 and P2:
            \begin{enumerate}
              \item Copy a random segment from P1.
              \item Starting from the first crossover point, look for elements in P2 that have not been copied.
              \item For each of these elements $i$, look in the offspring to see which element $j$ has been copied in its place.
              \item Place $i$ in the position occupied by $j$ in P2. If the place occupied by $j$ in P2 has already been filled in by $k$ put $i$ in the position occupied by $k$ in P2.
              \item The rest of the offspring can be filled in from P2.
            \end{enumerate}
          \item \textbf{Edge crossovers}
        \end{itemize}

        Below are some crossover operations than can be applied to order-based problems:
        \begin{itemize}
          \item \textbf{Order 1 Crossover}, which follows the approach below to generate children:
            \begin{enumerate}
              \item Choose arbitrary part from P1.
              \item Copy this part to first child.
              \item Starting from the right of the cut point of the copied part, copy the elements in the order of P2 that are not yet in the child, wrapping around if needed.
            \end{enumerate}
          \item \textbf{Cycle Crossover}, which follows the approach below to generate children:
            \begin{enumerate}
              \item Form a cycle of alleles from P1 by:
                \begin{enumerate}
                  \item Start with the first allele of P1.
                  \item Go to the position in P1 that has the value of the corresponding allele in P2.
                  \item Add this allele to the cycle.
                  \item Repeat this cycle formation until the first allele of P1 is arrived at.
                \end{enumerate}
              \item Put alleles of the cycle in the in the positions from P1.
              \item Repeat steps above for second parent.
            \end{enumerate}
        \end{itemize}

      \subsubsection{Mutation}
        Some previously defined mutation operators may lead to inadmissible solutions for permutation problems. Below are some mutations that can be applied:
        \begin{itemize}
          \item \textbf{Insert mutations}, which pick two elements, and move one to immediately follow the other, shifting any elements if necessary (i.e. as in
            insertion sort)
          \item \textbf{Swap mutations}, which pick two elements and swap their order.
          \item \textbf{Inversion mutations}, which pick two genes and reverse the subsequence between them.
          \item \textbf{Scramble mutations}, which pick two genes and find a random permutation of the genes between them.
        \end{itemize}

    \subsection{Advantages and Disadvantages}
      A few advantages of GAs include that they are highly multimodal, can be applied to discrete and continuous problems, can support high-dimensional
      problems, and support non-linear dependencies between parameters. A few disadvantages of GAs include premature conversion, poor choice of genetic
      operators, and biased or incomplete representations.

\end{document}
